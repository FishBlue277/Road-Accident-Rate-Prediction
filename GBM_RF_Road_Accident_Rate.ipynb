{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w99x30GORa-E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
        "import warnings\n",
        "import optuna\n",
        "import scipy.stats as st\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "ETobP6N-R0jX",
        "outputId": "aefd88a7-d6a4-4810-8803-8a8e95f997ab"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(r\"C:\\Users\\jiahe\\OneDrive\\Desktop\\kaggle\\test.csv\")\n",
        "train = pd.read_csv(r\"C:\\Users\\jiahe\\OneDrive\\Desktop\\kaggle\\train.csv\")\n",
        "\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Test shape:\", test.shape)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "TNAxDkyWSUnL",
        "outputId": "38ada3a0-779a-4365-dac7-06a4b63cff26"
      },
      "outputs": [],
      "source": [
        "train.isnull().sum().sort_values(ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "V2Jr5HjoSYPF",
        "outputId": "d569ebb1-6560-46e9-9f92-8cd5598af41d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(train['accident_risk'], bins=30, kde=True, color='teal')\n",
        "plt.title(\"Distribution of Accident Risk\")\n",
        "plt.xlabel(\"accident_risk\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_cat = train.copy()\n",
        "test_cat = test.copy()\n",
        "\n",
        "cat_cols = ['road_type', 'lighting', 'weather', 'time_of_day']\n",
        "\n",
        "for col in cat_cols:\n",
        "    categories = pd.Categorical(pd.concat([train_cat[col], test_cat[col]], axis=0)).categories\n",
        "    train_cat[col] = pd.Categorical(train_cat[col], categories=categories)\n",
        "    test_cat[col] = pd.Categorical(test_cat[col], categories=categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of categorical columns to target encode\n",
        "target_encode_cols = ['road_type', 'lighting', 'weather', 'time_of_day']\n",
        "\n",
        "# Initialize KFold for target encoding\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "encoded_features = pd.DataFrame(index=train.index)\n",
        "\n",
        "for col in target_encode_cols:\n",
        "    print(f\"Target encoding: {col}\")\n",
        "    oof = pd.Series(np.nan, index=train.index)\n",
        "\n",
        "    # Create out-of-fold mean for each category\n",
        "    for train_idx, val_idx in kf.split(train):\n",
        "        means = train.iloc[train_idx].groupby(col)['accident_risk'].mean()\n",
        "        oof.iloc[val_idx] = train.iloc[val_idx][col].astype(str).map(means)\n",
        "\n",
        "\n",
        "    encoded_features[col + '_te'] = oof\n",
        "\n",
        "# Add encoded features to train\n",
        "train = pd.concat([train, encoded_features], axis=1)\n",
        "\n",
        "# Apply same encoding (using full-train mean) to test\n",
        "for col in target_encode_cols:\n",
        "    global_mean = train.groupby(col)['accident_risk'].mean()\n",
        "    test[col + '_te'] = test[col].astype(str).map(global_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# === Define baseline analytic function and clipper ===\n",
        "# =====================================================\n",
        "def f_base(X):\n",
        "    \"\"\"Simple analytic approximation of accident risk.\"\"\"\n",
        "    return (\n",
        "        0.3 * X[\"curvature\"]\n",
        "        + 0.2 * (X[\"lighting\"] == \"night\").astype(int)\n",
        "        + 0.1 * (X[\"weather\"] != \"clear\").astype(int)\n",
        "        + 0.2 * (X[\"speed_limit\"] >= 60).astype(int)\n",
        "        + 0.1 * (X[\"num_reported_accidents\"] > 2).astype(int)\n",
        "    )\n",
        "\n",
        "def clip(f, sigma=0.05):\n",
        "    \"\"\"Smoothly clip predictions into [0,1] using truncated normal expectation.\"\"\"\n",
        "    def clip_f(X):\n",
        "        mu = f(X)\n",
        "        a, b = -mu/sigma, (1 - mu)/sigma\n",
        "        Phi_a, Phi_b = st.norm.cdf(a), st.norm.cdf(b)\n",
        "        phi_a, phi_b = st.norm.pdf(a), st.norm.pdf(b)\n",
        "        return mu*(Phi_b - Phi_a) + sigma*(phi_a - phi_b) + 1 - Phi_b\n",
        "    return clip_f\n",
        "\n",
        "# Apply baseline to train and test\n",
        "train[\"y_base\"] = clip(f_base)(train)\n",
        "test[\"y_base\"] = clip(f_base)(test)\n",
        "\n",
        "# Compute residual target\n",
        "train[\"y_resid\"] = train[\"accident_risk\"] - train[\"y_base\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = train_cat.drop(columns=['id', 'accident_risk'])\n",
        "y = train['y_resid']\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importances = pd.DataFrame()\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    model_fs = LGBMRegressor(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.03,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model_fs.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[early_stopping(100)]\n",
        "    )\n",
        "\n",
        "    fold_importance = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'gain': model_fs.booster_.feature_importance(importance_type='gain'),\n",
        "        'fold': fold + 1\n",
        "    })\n",
        "    importances = pd.concat([importances, fold_importance], axis=0)\n",
        "\n",
        "# Compute feature importance\n",
        "importance_df = (\n",
        "    importances.groupby('feature')['gain']\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Filter weak features\n",
        "threshold = importance_df['gain'].median() * 0.1\n",
        "selected_features = importance_df.query('gain > @threshold')['feature'].tolist()\n",
        "\n",
        "print(f\"✅ Selected {len(selected_features)} / {len(X.columns)} features after multi-fold averaging.\")\n",
        "\n",
        "# Reduce the dataset to selected features\n",
        "X = X[selected_features]\n",
        "test_X = test_cat[selected_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": 8000,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.05, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 50, 200),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 0.9),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 200),\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "        \"verbosity\": -1\n",
        "    }\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    rmse_scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = LGBMRegressor(**params)\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_metric=\"rmse\",\n",
        "            callbacks=[optuna.integration.LightGBMPruningCallback(trial, \"rmse\")]\n",
        "        )\n",
        "        preds = model.predict(X_val)\n",
        "        mse = mean_squared_error(y_val, preds)\n",
        "        rmse = np.sqrt(mse)\n",
        "        rmse_scores.append(rmse)\n",
        "        \n",
        "    return np.mean(rmse_scores)\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=40, show_progress_bar=True)\n",
        "\n",
        "print(\"Best RMSE:\", study.best_value)\n",
        "print(\"Best Params:\", study.best_params)\n",
        "\n",
        "best_params = study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up 8-Fold CV\n",
        "kf = KFold(n_splits=8, shuffle=True, random_state=42)\n",
        "rmse_scores = []\n",
        "test_X = test_cat.drop(columns=['id'])\n",
        "test_preds_all = np.zeros(len(test_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# store out-of-fold preds for later alpha tuning\n",
        "oof_lgb = np.zeros(len(X))\n",
        "oof_y = y.values.copy()\n",
        "oof_rf = np.zeros(len(X))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    print(f\"\\n===== Fold {fold + 1} / {kf.n_splits} =====\")\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    # Align columns\n",
        "    X_val = X_val[X_train.columns]\n",
        "    test_X = test_X[X_train.columns]\n",
        "\n",
        "    # === LightGBM model (tuned) ===\n",
        "    model_lgb = LGBMRegressor(\n",
        "        **study.best_params,\n",
        "        n_estimators=12000,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbosity=-1\n",
        "    )\n",
        "\n",
        "    model_lgb.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[early_stopping(200), log_evaluation(0)]\n",
        "    )\n",
        "    \n",
        "    preds_lgb = model_lgb.predict(X_val)\n",
        "    oof_lgb[val_idx] = preds_lgb\n",
        "\n",
        "    # === Random Forest model ===\n",
        "    # === Prepare numeric data for Random Forest ===\n",
        "    X_train_rf = pd.get_dummies(X_train, drop_first=True)\n",
        "    X_val_rf   = pd.get_dummies(X_val, drop_first=True)\n",
        "\n",
        "    # Align columns (since get_dummies may create mismatched columns)\n",
        "    X_train_rf, X_val_rf = X_train_rf.align(X_val_rf, join='left', axis=1, fill_value=0)\n",
        "\n",
        "    model_rf = RandomForestRegressor(\n",
        "        n_estimators=500,\n",
        "        max_depth=15,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    model_rf.fit(X_train_rf, y_train)\n",
        "    preds_rf = model_rf.predict(X_val_rf)\n",
        "    oof_rf[val_idx] = preds_rf\n",
        "\n",
        "    # temporary equal blend for monitoring\n",
        "    val_preds = 0.5 * preds_lgb + 0.5 * preds_rf\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "    rmse_scores.append(rmse)\n",
        "    print(f\"Fold {fold + 1} RMSE: {rmse:.5f}\")\n",
        "\n",
        "    # === Predict on test and accumulate (equal blend for now) ===\n",
        "    test_X_rf = pd.get_dummies(test_X, drop_first=True)\n",
        "    test_X_rf = test_X_rf.reindex(columns=X_train_rf.columns, fill_value=0)\n",
        "\n",
        "    test_pred_lgb = model_lgb.predict(test_X)\n",
        "    test_pred_rf = model_rf.predict(test_X_rf)\n",
        "    test_preds_all += (0.5 * test_pred_lgb + 0.5 * test_pred_rf) / kf.n_splits\n",
        "\n",
        "# Display preliminary CV RMSE\n",
        "print(\"\\n============================\")\n",
        "print(f\"Average RMSE (equal 0.5 blend): {np.mean(rmse_scores):.5f}\")\n",
        "print(\"============================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "overall_corr = np.corrcoef(oof_lgb, oof_rf)[0, 1]\n",
        "print(f\"\\nOverall OOF correlation between LGBM and RF: {overall_corr:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.kdeplot(x=oof_lgb, y=oof_rf, fill=True, cmap='viridis')\n",
        "plt.title(f'OOF Prediction Correlation: {overall_corr:.4f}')\n",
        "plt.xlabel('LGBM OOF Predictions')\n",
        "plt.ylabel('RF OOF Predictions')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# === Step 2: Tune alpha weight between LGBM & RF ===\n",
        "# =====================================================\n",
        "def objective_alpha(trial):\n",
        "    alpha = trial.suggest_float(\"alpha\", 0.0, 1.0)\n",
        "    blend = alpha * oof_lgb + (1 - alpha) * oof_rf\n",
        "    return np.sqrt(mean_squared_error(oof_y, blend))\n",
        "\n",
        "study_alpha = optuna.create_study(direction=\"minimize\")\n",
        "study_alpha.optimize(objective_alpha, n_trials=40)\n",
        "best_alpha = study_alpha.best_params[\"alpha\"]\n",
        "print(f\"✅ Best alpha for blending: {best_alpha:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# === Step 3: Final blending using best alpha ===\n",
        "# =====================================================\n",
        "# recompute CV RMSE using best alpha\n",
        "final_oof = best_alpha * oof_lgb + (1 - best_alpha) * oof_rf\n",
        "final_rmse = np.sqrt(mean_squared_error(oof_y, final_oof))\n",
        "print(f\"Final blended OOF RMSE: {final_rmse:.5f}\")\n",
        "\n",
        "# re-blend test predictions using alpha\n",
        "test_pred_final = best_alpha * test_pred_lgb + (1 - best_alpha) * test_pred_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# === Optional Step: Ridge regression meta-blender ===\n",
        "# =====================================================\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "print(\"\\n--- Ridge Regression Meta-Blender ---\")\n",
        "\n",
        "meta_X = np.vstack([oof_lgb, oof_rf]).T\n",
        "meta_y = oof_y\n",
        "\n",
        "ridge = RidgeCV(alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 10])\n",
        "ridge.fit(meta_X, meta_y)\n",
        "ridge_oof = ridge.predict(meta_X)\n",
        "ridge_rmse = np.sqrt(mean_squared_error(meta_y, ridge_oof))\n",
        "print(f\"Ridge-blend OOF RMSE: {ridge_rmse:.5f}\")\n",
        "print(\"Ridge coefficients (LGBM, RF):\", ridge.coef_)\n",
        "\n",
        "# Predict on test set using same meta model\n",
        "test_meta_X = np.vstack([test_pred_lgb, test_pred_rf]).T\n",
        "test_pred_ridge = ridge.predict(test_meta_X)\n",
        "\n",
        "# Optionally compare with Optuna alpha blend\n",
        "print(f\"Optuna alpha-blend OOF RMSE: {final_rmse:.5f}\")\n",
        "if ridge_rmse < final_rmse:\n",
        "    print(\"✅ Ridge performs slightly better; using Ridge predictions.\")\n",
        "    test_pred_final = test_pred_ridge\n",
        "else:\n",
        "    print(\"ℹ️ Keeping Optuna alpha blend as final.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# === Step 5: Save OOF and test predictions for stacking ===\n",
        "# =====================================================\n",
        "\n",
        "# Save OOF predictions for meta-learning later\n",
        "oof_df = pd.DataFrame({\n",
        "    'id': train_cat['id'],\n",
        "    'oof_lgb': oof_lgb,\n",
        "    'oof_rf': oof_rf,\n",
        "    'oof_blend': final_oof,   # blended or ridge version\n",
        "    'y_true': oof_y\n",
        "})\n",
        "oof_df.to_csv('oof_lgb_rf.csv', index=False)\n",
        "print(\"✅ Saved OOF predictions to oof_lgb_rf.csv\")\n",
        "\n",
        "# Save test predictions (for later external blending)\n",
        "test_stack_df = pd.DataFrame({\n",
        "    'id': test_cat['id'],\n",
        "    'test_lgb': test_pred_lgb,\n",
        "    'test_rf': test_pred_rf,\n",
        "    'test_blend': test_pred_final\n",
        "})\n",
        "test_stack_df.to_csv('test_lgb_rf.csv', index=False)\n",
        "print(\"✅ Saved test predictions to test_lgb_rf.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# === Step 4: Save final blended predictions ===\n",
        "# =====================================================\n",
        "\n",
        "# Combine optimized test predictions with baseline offset\n",
        "final_preds = test_pred_final + test[\"y_base\"]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_cat['id'],\n",
        "    'accident_risk': final_preds\n",
        "})\n",
        "\n",
        "submission.to_csv('submission_rfblend.csv', index=False)\n",
        "print(\"✅ submission_rfblend.csv ready for Kaggle upload!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
